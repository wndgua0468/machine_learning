{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM/dS7KFDKK0/FwVwgj2OQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wndgua0468/machine_learning/blob/main/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5_14%EC%A3%BC%EC%B0%A8_Beans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E_pJJhSnMPC"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# COMPUTER VISION WITH CNNs\n",
        "#\n",
        "# Create and train a classifier to classify images between three categories\n",
        "# of beans using the beans dataset.\n",
        "# ==============================================================================\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Beans dataset has images belonging to 3 classes as follows:\n",
        "# 2 disease classes (Angular leaf spot, bean rust)\n",
        "# 1 healthy class (healthy).\n",
        "# The images are of different sizes and have 3 channels.\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (300,300,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification. You must\n",
        "#    resize all the images in the dataset to this size while pre-processing\n",
        "#    the dataset.\n",
        "# 2. The last layer of your model must be a Dense layer with 3 neurons\n",
        "#    activated by softmax since this dataset has 3 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.75 or above on the normalized validation dataset for top marks.\n",
        "#\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Use this constant wherever necessary\n",
        "IMG_SIZE = 300\n",
        "\n",
        "# This function normalizes and resizes the images.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # RESIZE YOUR IMAGES HERE (HINT: After resizing the shape of the images\n",
        "    # should be (300, 300, 3).\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255))\n",
        "    image1= tf.image.resize(image, size=(300,300) )\n",
        "    image2 = image1/255\n",
        "    return image2, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Loads and splits the data into training and validation splits using tfds.\n",
        "    (ds_train, ds_validation), ds_info = tfds.load(\n",
        "        name='beans',\n",
        "        split=['train', 'validation'],\n",
        "        as_supervised=True,\n",
        "        with_info=True)\n",
        "\n",
        "    BATCH_SIZE = 32 # YOUR CODE HERE\n",
        "\n",
        "    # Resizes and normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    ds_train = ds_train.map(preprocess).cache().shuffle(\n",
        "        ds_info.splits['train'].num_examples).batch(BATCH_SIZE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    ds_validation = ds_validation.map(preprocess).batch(BATCH_SIZE).cache(\n",
        "\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # If you don't adhere to the instructions in the following comments,\n",
        "        # tests will fail to grade your model:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (300,300,3).\n",
        "        # Make sure that your last layer has 3 (number of classes) neurons\n",
        "        # activated by softmax.\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        # 2D -> 1D로 변환을 위하여 Flatten 합니다.\n",
        "        Flatten(),\n",
        "        # 과적합 방지를 위하여 Dropout을 적용합니다. 학습시에 큰 노드 50%만 사용\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(\n",
        "        # YOUR CODE HERE\n",
        "        optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']\n",
        "    )\n",
        "    epochs = 10\n",
        "    model.fit(\n",
        "        # YOUR CODE HERE\n",
        "     ds_train,\n",
        "                    validation_data=(ds_validation),\n",
        "                    epochs=epochs\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel_3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import tensorflow_datasets as tfds\n",
        "datasets , info = tfds.load(name = 'beans', with_info = True,\n",
        "                            as_supervised = True,\n",
        "                            split = ['train', 'test', 'validation']) "
      ],
      "metadata": {
        "id": "yE1tanvmrrf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, info_train = tfds.load(name = 'beans',\n",
        "                              with_info=True, split = 'test'\n",
        "                              )"
      ],
      "metadata": {
        "id": "Y_n_dJNuhYpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.show_examples(train, info_train)"
      ],
      "metadata": {
        "id": "lI0jJte5hriU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
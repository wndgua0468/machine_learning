{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOLPu3ExNB0j2nxbji8lCZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wndgua0468/machine_learning/blob/main/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5_10%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 자연어 처리\n",
        "1.자연어 처리 방법\n",
        "1.토큰화(문장을 띄어쓰기 기준으로 나눔) + 단어 사전 매핑(단어와 숫자매칭)\n",
        "\n",
        "한국어 감정분석\n",
        "1. 더이터 불러오기\n",
        "2. 탐색적 데이터 불러오기"
      ],
      "metadata": {
        "id": "OVf6VB-mzUmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB4aGmGAscWK"
      },
      "outputs": [],
      "source": [
        "# 필요 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 불러오기"
      ],
      "metadata": {
        "id": "zoBOTLf53CfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naver sentiment movie corpus v1.0 데이터 불러오기\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    'ratings_train.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', extract=True)\n",
        "\n",
        "train = pd.read_csv(train_file, sep='\\t')"
      ],
      "metadata": {
        "id": "kT_c0wHS0l6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA (탐색적 데이터 분석)"
      ],
      "metadata": {
        "id": "oxkxUqK73GZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 크기 및 샘플 확인\n",
        "print(\"train shape: \", train.shape)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "FHuv3Hlx0sUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train)"
      ],
      "metadata": {
        "id": "p7-S2PFs03E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블별 개수\n",
        "cnt = train['label'].value_counts()\n",
        "print(cnt)"
      ],
      "metadata": {
        "id": "481_fvgX08zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블별 비율 \n",
        "sns.countplot(x='label',data=train)"
      ],
      "metadata": {
        "id": "cYASpSXo1LMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "train.isnull().sum() "
      ],
      "metadata": {
        "id": "GWquuwqm1TR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치(의견없음)가 특정 label값만 있는지 확인\n",
        "train[train['document'].isnull()]"
      ],
      "metadata": {
        "id": "w-GGWb8A1YVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 별 텍스트 길이\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "data_len=train[train['label']==1]['document'].str.len()\n",
        "ax1.hist(data_len)\n",
        "ax1.set_title('positive')\n",
        "\n",
        "data_len=train[train['label']==0]['document'].str.len()\n",
        "ax2.hist(data_len)\n",
        "ax2.set_title('negative')\n",
        "fig.suptitle('Number of characters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MeMcZ0Gt1azg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "형태소 분석기 불러오기"
      ],
      "metadata": {
        "id": "c-cCpuvM20PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "metadata": {
        "id": "idgDWknS1xIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "metadata": {
        "id": "ZtyNy8qz3NKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! bash install_mecab-ko_on_colab_light_220429.sh"
      ],
      "metadata": {
        "id": "nlg0NkUt3T2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mecab = Mecab()\n",
        "print(mecab.pos)"
      ],
      "metadata": {
        "id": "777Zvx-f8fnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma, Komoran, Okt, Mecab 형태소\n",
        "import konlpy\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "-fS57_B_7T9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소별 샘플\n",
        "text = \"영실아안녕오늘날씨어때?\"\n",
        "\n",
        "def sample_ko_pos(text):\n",
        "    print(f\"==== {text} ====\")\n",
        "    print(\"kkma:\",kkma.pos(text))\n",
        "    print(\"komoran:\",komoran.pos(text))\n",
        "    print(\"okt:\",okt.pos(text))\n",
        "    print(\"mecab:\",mecab.pos(text))\n",
        "    print(\"\\n\")\n",
        "\n",
        "sample_ko_pos(text)"
      ],
      "metadata": {
        "id": "sTgms0OT7cvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리"
      ],
      "metadata": {
        "id": "E6dcRa2-9ZrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리(영어와 한글만 남기고 삭제)\n",
        "train['document'] = train['document'].str.replace(\"[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "train['document'].head()"
      ],
      "metadata": {
        "id": "nFkGcieV9Uga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 제거\n",
        "train = train.dropna()\n",
        "train.shape"
      ],
      "metadata": {
        "id": "fQQIKghy9gPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스탑워드와 형태소 분석\n",
        "def word_tokenization(text):\n",
        "  stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도'] # 한글 불용어\n",
        "  return [word for word in mecab.morphs(text) if word not in stop_words]"
      ],
      "metadata": {
        "id": "kz2XtVIh9j1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = train['document'].apply((lambda x: word_tokenization(x)))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "jA9EDNDL-Ce4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 validation 분할\n",
        "\n",
        "training_size = 120000\n",
        "\n",
        "# train 분할\n",
        "train_sentences = data[:training_size]\n",
        "valid_sentences = data[training_size:]\n",
        "\n",
        "# label 분할\n",
        "train_labels = train['label'][:training_size]\n",
        "valid_labels = train['label'][training_size:]"
      ],
      "metadata": {
        "id": "aYbzIfVT-G_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# vocab_size 설정\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "print(\"총 단어 갯수 : \",len(tokenizer.word_index))\n",
        "\n",
        "# 5회 이상만 vocab_size에 포함\n",
        "def get_vocab_size(threshold):\n",
        "  cnt = 0\n",
        "  for x in tokenizer.word_counts.values():\n",
        "    if x >= threshold:\n",
        "      cnt = cnt + 1\n",
        "  return cnt\n",
        "\n",
        "vocab_size = get_vocab_size(5) # 5회 이상 출현 단어\n",
        "print(\"vocab_size: \", vocab_size)"
      ],
      "metadata": {
        "id": "eE3GkXFt_ws4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_tok = \"\" # 사전에 없는 단어\n",
        "vocab_size = 15000\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(data)\n",
        "print(tokenizer.word_index)\n",
        "print(\"단어 사전 개수:\", len(tokenizer.word_counts))"
      ],
      "metadata": {
        "id": "7BCPalUsAgRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자를 숫자로 표현\n",
        "print(train_sentences[:2])\n",
        "print(valid_sentences[:2])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
        "print(train_sequences[:2])\n",
        "print(valid_sentences[:2])"
      ],
      "metadata": {
        "id": "8nyxWiEiAmqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장의 최대 길이\n",
        "max_length = max(len(x) for x in train_sequences)\n",
        "print(\"문장 최대 길이:\", max_length)"
      ],
      "metadata": {
        "id": "SQQ9e_poAywP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 길이를 동일하게 맞춘다\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "valid_padded = pad_sequences(valid_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "train_labels = np.asarray(train_labels).reshape(-1,1)\n",
        "valid_labels = np.asarray(valid_labels).reshape(-1,1)\n",
        "\n",
        "print(\"샘플:\", train_padded[:1])"
      ],
      "metadata": {
        "id": "lgonZgN2A4Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "양방향 LSTM 모델\n",
        "1.\n",
        "2.\n",
        "3\n",
        "4."
      ],
      "metadata": {
        "id": "UrlfKEgaBCwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "                Embedding(vocab_size, 32),\n",
        "                Bidirectional(LSTM(32, return_sequences=True)),    \n",
        "                Dense(32, activation='relu'),\n",
        "                Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hHPzmI0gBAwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 좋은 loss의 가중치 저장\n",
        "checkpoint_path = 'best_performed_model.ckpt'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                save_weights_only=True, \n",
        "                                                save_best_only=True, \n",
        "                                                monitor='val_loss',\n",
        "                                                verbose=1)"
      ],
      "metadata": {
        "id": "1EyHGiggDCGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습조기종료\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "# 학습\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                validation_data=(valid_padded, valid_labels), \n",
        "                callbacks=[early_stop, checkpoint], batch_size=64, epochs=10, verbose=2)"
      ],
      "metadata": {
        "id": "bwdnPCpBDFNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가"
      ],
      "metadata": {
        "id": "qxYz1l1-EgOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "CC_3hxfmEc8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "U1zc48ZsGMxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 불러오기\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    'ratings_test.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt', extract=True)\n",
        "\n",
        "test = pd.read_csv(test_file, sep='\\t')\n",
        "test.head()"
      ],
      "metadata": {
        "id": "KeEJrAUMGZs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def preprocessing(df):\n",
        "  df['document'] = df['document'].str.replace(\"[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "  df = df.dropna()\n",
        "  test_label = np.asarray(df['label'])\n",
        "  test_data =  df['document'].apply((lambda x: word_tokenization(x)))\n",
        "  test_data = tokenizer.texts_to_sequences(test_data)\n",
        "  test_data = pad_sequences(test_data, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "  return test_data, test_label\n",
        "\n",
        "test_data, test_label = preprocessing(test)\n",
        "print(model.evaluate(test_data, test_label))"
      ],
      "metadata": {
        "id": "vIzrAxngGdNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "저장된 모델 불러오기"
      ],
      "metadata": {
        "id": "rZPD0TSLGsfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 모델 로드 후 평가\n",
        "model2 = create_model()\n",
        "model2.evaluate(test_data, test_label)"
      ],
      "metadata": {
        "id": "c1S-jc6KG0Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 가중치 적용된 모델 로드 후 평가\n",
        "model2.load_weights(checkpoint_path)\n",
        "model2.evaluate(test_data, test_label)"
      ],
      "metadata": {
        "id": "Xc2tnLS_G3WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측하기"
      ],
      "metadata": {
        "id": "lei987ykN0XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "fSQk3CUFMHeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt() ##Open Korean Text 트위터에서 만든 형태소 분석기\n",
        "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔',stem = True)"
      ],
      "metadata": {
        "id": "eRy291E0OWC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##문자열 치환 re.sub(검색패턴, 변경하고 싶은 문자, 검색되는 문자열) \n",
        "new_sentence = '이 영화 개꿀잼ㅎㅎㅎㅋㅋㅋ'\n",
        "new_sentence = re.sub(r'[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]','',new_sentence)\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "_vnaYO8MOji5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = okt.morphs(new_sentence, stem=True)\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "qj-XacFJOweQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도']"
      ],
      "metadata": {
        "id": "R9gaxzvnPQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = [word for word in new_sentence if not word in stop_words]\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "Vjp27eoFPZWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# vocab_size 설정\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "VBEHLk5nPcqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_tok = \"\"#사전에 없는 단어\n",
        "vocab_size = 15000\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(new_sentence)"
      ],
      "metadata": {
        "id": "kY_Wa1PqP9Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences(new_sentence)\n",
        "encoded"
      ],
      "metadata": {
        "id": "mqNHcf8hQeNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_new=pad_sequences(encoded, maxlen=max_length)\n",
        "pad_new.shape"
      ],
      "metadata": {
        "id": "iNYwTltZQiPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(pad_new).shape"
      ],
      "metadata": {
        "id": "91-Fjh0iQyYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(pad_new)"
      ],
      "metadata": {
        "id": "-6yTFj3YQ1sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=np.max(model2.predict(pad_new))\n",
        "print(score)"
      ],
      "metadata": {
        "id": "A8qCbj40UzKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 74\n",
        "stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도']\n",
        "\n",
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = new_sentence.replace(\"[^A-Za-z-가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) #토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stop_words] #불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences(new_sentence) #정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) #패딩\n",
        "  score = np.max(model2.predict(pad_new)) #예측\n",
        "  if score > 0.5 :\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰 입니다.\\n\".format(score*100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰 입니다.\\n\".format(score*100))"
      ],
      "metadata": {
        "id": "W9qMC5XAU2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ~~!!')"
      ],
      "metadata": {
        "id": "IY7GxILJU7XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
      ],
      "metadata": {
        "id": "kJHDhJcvVSoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "metadata": {
        "id": "i9QaSLFFVTv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('감독 뭐하는 놈이냐?')"
      ],
      "metadata": {
        "id": "3DwXtzWuVYBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
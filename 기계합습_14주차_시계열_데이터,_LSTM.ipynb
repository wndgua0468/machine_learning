{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNeoR7EhcImiXExrF8RZEVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wndgua0468/machine_learning/blob/main/%EA%B8%B0%EA%B3%84%ED%95%A9%EC%8A%B5_14%EC%A3%BC%EC%B0%A8_%EC%8B%9C%EA%B3%84%EC%97%B4_%EB%8D%B0%EC%9D%B4%ED%84%B0%2C_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L99YsfHKqeP9"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# TIME SERIES QUESTION 시계열 데이터, LSTM\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# =================================================== #\n",
        "# 문제명: Category 5 - Weekly US Retail Price\n",
        "# val_mae: 0.026 혹은 그 이하\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "    \n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    # Code to train and compile the model\n",
        "    optimizer =  tf.keras.optimizers.Adam(0.00003)  # YOUR CODE HERE\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
        "\n",
        "    model.fit(train_set, validation_data=(valid_set), epochs=150, callbacks=[checkpoint])\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"model5.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##데이터 확인하기\n",
        "\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "XrD_FbufrM9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.dropbox.com/s/eduk281didil1km/Weekly_U.S.Diesel_Retail_Prices.csv?dl=1\"\n",
        "urllib.request.urlretrieve(url, 'Weekly_U.S.Diesel_Retail_Prices.csv')"
      ],
      "metadata": {
        "id": "bRlacGHmre_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"Weekly_U.S.Diesel_Retail_Prices.csv\", infer_datetime_format=True,\n",
        "            index_col='Week of', header=0)"
      ],
      "metadata": {
        "id": "SbzOgAxerpPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}